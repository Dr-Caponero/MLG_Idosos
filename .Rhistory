<<<<<<< HEAD
covid_data <- mdmr::regioes
mdmModel <- mdm_score(covid_data, GOLB_print = TRUE)
adj_matrix_GOB <- mdmr::run_BN_pygobnilp("mdm_score_07_ago_2023", palim = 5)
reticulate::py_last_error()
adj_matrix_GOB <- mdmr::run_BN_pygobnilp("mdm_score_07_ago_2023", palim = 5)
adj_matrix_GOB <- mdmr::run_BN_pygobnilp("mdm_score_07_ago_2023", palim = 5)
reticulate::py_install('pygobnilp')
reticulate::use_virtualenv("r-reticulate")
reticulate::virtualenv_list()
adj_matrix_GOB <- mdmr::run_BN_pygobnilp("mdm_score_07_ago_2023", palim = 5)
reticulate::py_install('_graphviz')
reticulate::py_install('graphviz')
adj_matrix_GOB <- mdmr::run_BN_pygobnilp("mdm_score_07_ago_2023", palim = 5)
reticulate::py_last_error()
reticulate::import("pygobnilp")
reticulate::virtualenv_list()
reticulate::import("pygobnilp.gobnilp")
adj_matrix_GOB <- mdmr::run_BN_pygobnilp("mdm_score_07_ago_2023", palim = 5)
adj_matrix_GOB <- mdmr::run_BN_pygobnilp("mdm_score_07_ago_2023", palim = 5)
#--------------------------------------------------/----------------------------------------
set.seed(30)
n=100
theta_ant = c(0,0,0)
y=matrix(,nrow=n,ncol=2)
for (i in seq(1,n)){
theta_i = theta_ant + rnorm(3,0,0.01)
y_1i =  theta_i[1] + rnorm(1,0,0.01)
y_2i = theta_i[2] + theta_i[3]*y_1i + rnorm(1,0,0.01)
y[i,] = c(y_1i,y_2i)
theta_ant = theta_i
}
y = data.frame(y)
colnames(y) = c('Y1','Y2')
cor(y)
y
set.seed(2)
nt = 200
th <- array(0, dim=c(3,nt))
for (i in 2:nt){
th[,i] = th[,i-1] + rnorm(3, mean = 0.0, sd = 0.1)
}
y1 = th[1,] + rnorm(nt, mean = 0.0, sd = 1)
y2 = th[2,] + th[3,]*y1 + rnorm(nt, mean = 0.0, sd = 1)
dts = cbind(y1,y2)
dts
sim_score <- mdm_score(dts,GOLB_print = TRUE)
library(devtools)
devtools::install_github("arzevedo/mdmr")
reticulate::install_python()
reticulate::virtualenv_install("r-reticulate", "pygobnilp")
#reticulate::virtualenv_list()
#reticulate::py_install('graphviz')
reticulate::use_virtualenv("r-reticulate")
library(mdmr)
sim_score <- mdm_score(dts,GOLB_print = TRUE)
sim_score
adj_matrix_sim <- mdmr::run_BN_pygobnilp('mdm_score_11_out_2023', palim_nodes  = 2)
sim_score
#Functions for MDM analysis
# Lilia Costa
# 17/02/13
# Packages
library(fpc)
############################################################################################
# Creating function for DLM with Filtering for unknown observational and state variances
############################################################################################
# Input:
# Yt = the vector of observed time series with length T
# Ft = the matrix of covariates with dimension: number of thetas (p) X sample size (T)
# delta = discount factor | Wt=Ctx(1-delta)/delta
# Gt = the matrix of state equation with dimension: p X p X T. The default is identity matrix block.
# m0 = the vector of prior mean at time t=0 with length p. The default is non-informative prior, with zero mean.
# CS0 = the squared matrix of prior variance - C*0 | C*0Vt = C0, with length p. The default is non-informative prior, with prior variance equal to 3 times the observed variance.
# n0 and d0 = the prior hypermarameters of precision phi ~ G(n0/2; d0/2). The default is non-informative prior, with value of 0.001. n0 has to be higher than 0.
# output:
# mt = the matrix of posterior mean with dimension p X T
# Ct = the squared matrix of posterior variance with dimension p X p X T
# Rt = the squared matrix of prior variance with dimension p X p X T
# nt and dt = the vector of prior hypermarameters of precision phi with length T
# ft = the vector of one-step forecast mean with length T
# Qt = the vector of one-step forecast variance with length T
# ets = the vector of standardised residuals with length T
# lpl = Log Predictive Likelihood with length T
dlm_filt <- function(Yt, Ft, delta, Gt = array(diag(nrow(Ft)), dim=c(nrow(Ft),nrow(Ft),length(Yt))), m0 = rep(0,nrow(Ft)), CS0 = 3*diag(nrow(Ft)), n0 = 0.001, d0 = 0.001) {
# defining objects
p = nrow(Ft) # the number of thetas
Nt = length(Yt)+1 # the sample size + t=0
if (n0 == 0){
n0 = 0.001
warning("n0 is 0.001")
}
Y = rep(0, Nt)
Y[2:Nt] = Yt
F = array(0, dim=c(p,Nt))
F[,2:Nt] = Ft
G = array(0, dim=c(p,p,Nt))
G[,,2:Nt] = Gt
mt = array(0, dim=c(p,Nt))
mt[,1] = m0
Ct = array(0, dim=c(p,p,Nt))
Ct[,,1] = CS0*d0/n0
Rt = array(0, dim=c(p,p,Nt))
nt = rep(0, Nt)
nt[1] = n0
dt = rep(0, Nt)
dt[1] = d0
ft = rep(0, Nt)
Qt = rep(0, Nt)
ets = rep(0, Nt)
lpl = rep(0, Nt)
for (i in 2:Nt){
# Posterior at {t-1}: (theta_{t-1}|y_{t-1}) ~ t_{n_{t-1}}[m_{t-1}, C_{t-1} = C*_{t-1}xd_{t-1}/n_{t-1}]
# Prior at {t}: (theta_{t}|y_{t-1}) ~ t_{n_{t-1}}[a_{t}, R_{t}]
at = G[,,i] %*% mt[,(i-1)]
RSt = G[,,i] %*% (Ct[,,(i-1)]*nt[(i-1)]/dt[(i-1)]) %*% t(G[,,i]) / delta
Rt[,,i] = RSt * dt[(i-1)] / nt[(i-1)]
# One-step forecast: (Y_{t}|y_{t-1}) ~ t_{n_{t-1}}[f_{t}, Q_{t}]
ft[i] = t(F[,i]) %*% at
QSt = t(F[,i]) %*% RSt %*% F[,i] + 1
Qt[i] = QSt * dt[(i-1)] / nt[(i-1)]
et = Y[i] - ft[i]
ets[i] = et / sqrt(Qt[i])
# Posterior at t: (theta_{t}|y_{t}) ~ t_{n_{t}}[m_{t}, C_{t}]
At = Rt[,,i] %*% F[,i] / Qt[i]
mt[,i] = at + At * et
nt[i] = nt[(i-1)] + 1
dt[i] = dt[(i-1)] + (et^2) / QSt
CSt = RSt - (At %*% t(At)) * QSt[1]
Ct[,,i] = CSt * dt[i] / nt[i]
# Log Predictive Likelihood
lpl[i] <- lgamma((nt[i]+1)/2)-lgamma(nt[i]/2)-0.5*log(pi*nt[i]*Qt[i])-((nt[i]+1)/2)*log(1+(1/nt[i])*et^2/Qt[i])
}
result <- list(mt=mt[,2:Nt], Ct=Ct[,,2:Nt], Rt=Rt[,,2:Nt], nt=nt[2:Nt], dt=dt[2:Nt], ft=ft[2:Nt], Qt=Qt[2:Nt], ets=ets[2:Nt], lpl=lpl[2:Nt])
return(result)
}
############################################################################################
# Creating function for DLM with Smoothing for unknown observational and state variances
############################################################################################
# Input: all objects are resulted from "dlm_filt", except Gt
# mt = the matrix of posterior mean with dimension p X T
# Ct = the squared matrix of posterior variance with dimension p X p X T
# Rt = the squared matrix of prior variance with dimension p X p X T
# nt and dt = the vector of prior hypermarameters of precision phi with length T
# Gt = the matrix of state equation with dimension: p X p X T. The default is identity matrix block.
# output:
# smt = the matrix of smoothing posterior mean with dimension p X T
# sCt = the squared matrix of smoothing posterior variance with dimension p X p X T
dlm_smoo <- function(mt, Ct, Rt, nt, dt, Gt = 0) {
# defining objects
if (is.vector(mt)){
mt = array(mt, dim=c(1,length(mt)))
Ct = array(Ct, dim=c(1,1,length(mt)))
Rt = array(Rt, dim=c(1,1,length(Rt)))
}
if (Gt == 0){Gt = array(diag(nrow(mt)), dim=c(nrow(mt),nrow(mt),ncol(mt)))}
p = nrow(mt) # the number of thetas
Nt = ncol(mt) # the sample size
smt = array(0, dim=c(p,Nt))
sCt = array(0, dim=c(p,p,Nt))
# in the last time point
smt[,Nt] = mt[,Nt]
sCt[,,Nt] = Ct[,,Nt]
# for other time points
for (i in (Nt-1):1){
RSt = Rt[,,(i+1)]*nt[i]/dt[i]
CSt = Ct[,,i]*nt[i]/dt[i]
inv.sR = solvecov(RSt, cmax = 1e+10)$inv
B = CSt %*% t(Gt[,,(i+1)]) %*% inv.sR
smt[,i] = mt[, i] + B %*% (smt[,(i+1)] - Gt[,,(i+1)] %*% mt[,i])
sCS = CSt + B %*% (sCt[,,(i+1)]*nt[Nt]/dt[Nt] - RSt) %*% t(B)
sCt[,,i] = sCS * dt[Nt] / nt[Nt]
}
result <- list(smt=smt, sCt=sCt)
return(result)
}
###################################################################
### Choosing the delta
###################################################################
# Input:
#  dts = the matrix with dataset; Number of timepoints X Number of nodes
#  m_ad = # Square Matrix Adjacent with dimension = Number of nodes # 1 if edge exists; 0 otherwise
#  nbf => the Log Predictive Likelihood will be calculate from this time point. It has to be a positive integer number. The default is 15.
#  delta = the vector with the sequence of all discount factors. The default is seq(from=0.5, to=1.0, by=0.01).
# Output:
# lpldet = LPL for each value of delta and for each node; length(delta) X Number of nodes;
# DF_hat = vector with delta that maximizes the LPL for each node with dimension = Number of nodes.
CDELT <- function(dts,m_ad,nbf=15,delta=seq(from=0.5, to=1.0, by=0.01)) {
nd = length(delta)
Nn = ncol(dts)
Nt = nrow(dts)
lpldet = array(0,dim=c(nd,Nn))
for (k in 1:nd){
for (i in 1:Nn){
# Initials:
p = sum(m_ad[,i])
if (m_ad[i,i] == 0) {p = p + 1}
Ft = array(1, dim=c(Nt,p))
aux = c(1:Nn)[m_ad[,i]>0]
aux2 = aux[aux!=i]
if (length(aux2)>0){ Ft[,2:(length(aux2)+1)] = dts[,aux2]}
Yt = dts[,i]
# DLM
a=dlm_filt(Yt, t(Ft), delta=delta[k])
lpldet[k,i]=sum(a$lpl[nbf:Nt])
}
}
#DF_hat=delta[max.col(t(lpldet))] # with some deltas provide NA in lpl, it means that this delta is not good for this particular dataset so we have to use:
DF_hat = rep(0,Nn)
for (i in 1:Nn){
DF_hat[i] = na.omit(delta[lpldet[,i]==max(lpldet[,i],na.rm=TRUE)])[1]
}
result <- list(lpldet = lpldet, DF_hat = DF_hat)
return(result)
}
############################################################################################
# Creating function for MDM with Filtering for unknown observational and state variances
############################################################################################
# Input:
# dts = the matrix with dataset; Number of timepoints X Number of nodes
# m_ad = # Square Matrix Adjacent with dimension = Number of nodes # 1 if edge exists; 0 otherwise
# DF_hat = vector with delta that maximizes the LPL for each node with dimension = Number of nodes.
# output:
# mt = list with the matrix of posterior mean with dimension p X T
# Ct = list with the squared matrix of posterior variance with dimension p X p X T
# Rt = list with the squared matrix of prior variance with dimension p X p X T
# nt and dt = list with the vector of prior hypermarameters of precision phi with length T
# ft = list with the vector of one-step forecast mean with length T
# Qt = list with the vector of one-step forecast variance with length T
# ets = list with the vector of standardised residuals with length T
# lpl = list with Log Predictive Likelihood with length T
mdm_filt <- function(dts, m_ad, DF_hat) {
Nn = ncol(dts)
Nt = nrow(dts)
mt = vector(Nn, mode = "list")
Ct = vector(Nn, mode = "list")
Rt = vector(Nn, mode = "list")
nt = vector(Nn, mode = "list")
dt = vector(Nn, mode = "list")
ft = vector(Nn, mode = "list")
Qt = vector(Nn, mode = "list")
ets = vector(Nn, mode = "list")
lpl = vector(Nn, mode = "list")
for (i in 1:Nn){
# Initials:
p = sum(m_ad[,i])
if (m_ad[i,i] == 0) {p = p + 1}
Ft = array(1, dim=c(Nt,p))
aux = c(1:Nn)[m_ad[,i]>0]
aux2 = aux[aux!=i]
if (length(aux2)>0){ Ft[,2:(length(aux2)+1)] = dts[,aux2]}
Yt = dts[,i]
# DLM
a=dlm_filt(Yt, t(Ft), delta=DF_hat[i])
mt[[i]] = a$mt
Ct[[i]] = a$Ct
Rt[[i]] = a$Rt
nt[[i]] = a$nt
dt[[i]] = a$dt
ft[[i]] = a$ft
Qt[[i]] = a$Qt
ets[[i]] = a$ets
lpl[[i]] = a$lpl
}
result <- list(mt=mt, Ct=Ct, Rt=Rt, nt=nt, dt=dt, ft=ft, Qt=Qt, ets=ets, lpl=lpl)
return(result)
}
############################################################################################
# Creating function for MDM with Smoothing for unknown observational and state variances
############################################################################################
# Input: all objects are resulted from "mdm_filt"
# mt = list with the matrix of posterior mean with dimension p X T
# Ct = list with the squared matrix of posterior variance with dimension p X p X T
# Rt = list with the squared matrix of prior variance with dimension p X p X T
# nt and dt = list with the vector of prior hypermarameters of precision phi with length T
# output:
# smt = list with the matrix of smoothing posterior mean with dimension p X T
# sCt = list with the squared matrix of smoothing posterior variance with dimension p X p X T
mdm_smoo <- function(mt, Ct, Rt, nt, dt) {
Nn = length(mt) # the number of nodes
smt = vector(Nn, mode = "list")
sCt = vector(Nn, mode = "list")
for (i in 1:Nn){
a=dlm_smoo(mt=mt[[i]], Ct=Ct[[i]], Rt=Rt[[i]], nt=nt[[i]], dt=dt[[i]])
smt[[i]] = a$smt
sCt[[i]] = a$sCt
}
result <- list(smt=smt, sCt=sCt)
return(result)
}
###################################################################
### Creating a file with structure for James' programm - GOBNILP
###################################################################
# Input:
#  dts => the matrix with dataset; Number of subjects X Number of timepoints X Number of nodes
#  nbf => the Log Predictive Likelihood will be calculate from this time point. It has to be a positive integer number. The default is 15.
#  delta = the vector with the sequence of all discount factors. The default is seq(from=0.5, to=1.0, by=0.01).
# Output:
# all.score => list with the first dimension is the subject and the second one is the row of structured file
# DF_hat => matrix with dimension Number of subject X Number of possible models X (Nn+2), where third dimension represents the chosen DF (or delta parameter) that maximizes the lpl; node; and his parents. The value "-9" represents null value.
#To export...
#  fn <- "subj"
#  for(i in 1:dim(dts)[1]){ write.table(a$all.score[[i]], paste(fn, i, "txt", sep = "."),quote = FALSE, row.names = FALSE, col.names = FALSE)}
mdm_score <- function(dts, nbf=15, delta=seq(from=0.5, to=1.0, by=0.01)) {
Ns = dim(dts)[1] # the number of subjects
Nt = dim(dts)[2] # the number of timepoints
Nn = dim(dts)[3] # the number of nodes
Nd =  Nn*2^(Nn-1)# the number of possible parent-child
delt_hat = array(0,dim=c(Ns,Nd)) # the discount factor chosen for each possibility
lpl = array(0,dim=c(Ns,Nd)) # scores
par_chil = array(-9,dim=c(Ns,Nd,(Nn+2))) #col1=subject; col2=model; col3={child,number of parents,parents}
# generating all possible combinations
cc = vector(Nn, mode = "list")
for (i in 1:Nn){
cc[[i]] = combn(c(1:Nn),i)
}
# finding the scores
ptm <- proc.time() # Start the clock!
for (s in 1:Ns){
#cat("loop subject ",s,"/",Ns, "\n")
Np=0
for (i in seq(from=1,to=Nn,by=2)){
cat("loop ",s,"/",Ns," and ",i,"/",Nn,"\n")
for (j in 1:ncol(cc[[i]])){
# Matrix Adjacent: 1 for edge; 0 otherwise
m_ad = diag(Nn)
p=cc[[i]][,j]
m_ad[p,]=1
# choosing the Discount Factor and finding the scores
delt_dag = CDELT(dts[s,,],m_ad, nbf=nbf, delta=delta)
delt_hat[s,(Np+1):(Np+Nn)]=delt_dag$DF_hat
#aux=t(delt_dag$lpldet)
#lpl[s,(Np+1):(Np+Nn)]=diag(aux[,max.col(aux)])
lpl[s,(Np+1):(Np+Nn)]=apply(delt_dag$lpldet,2,max,na.rm=TRUE) #deleting the NA from lpl
# child
par_chil[s,(Np+1):(Np+Nn),1] = seq(1:Nn)-1
# number of parents
par_chil[s,(Np+1):(Np+Nn),2] = i
par_chil[s,c((Np+1):(Np+Nn))[p],2] = i-1
# parents
par_chil[s,(Np+1):(Np+Nn),3:(2+i)] = t(array((p-1), dim=c(i,Nn)))
if (i==1){par_chil[s,c((Np+1):(Np+Nn))[p],3:(2+i)] = -9}
else {diag(par_chil[s,c((Np+1):(Np+Nn))[p],3:(2+i)]) = -9}
Np=Np+Nn
}
}
}
proc.time() - ptm # Stop the clock
DF_hat= array(-9,dim=c(Ns,Nd,(Nn+2)))
DF_hat[,,1]=delt_hat
DF_hat[,,2]=par_chil[,,1]
DF_hat[,,3:(Nn+2)]=par_chil[,,3:(Nn+2)]
dimnames(DF_hat)=list(c(1:Ns),c(1:Nd),c("DF","node",1:Nn))
# creating the file with structure of James'programm
all.score = list()
for (s in 1:Ns){
all.score[[s]] = array(" ",dim=c((Nd+Nn+1),(Nn+1)))
all.score[[s]][1,1] = Nn
a = cbind(lpl[s,],par_chil[s,,])
b=a[order(a[,2]),]
more=2
j=0
for (i in 1:Nd){
if (j==b[i,2]){
all.score[[s]][more,1:2] = c(j,Nd/Nn)
j=j+1
more=more+1
}
d = b[i,c(1,3:(Nn+3))]
all.score[[s]][more,1:length(d[d!=-9])] = d[d!=-9]
more = more + 1
}
}
result <- list(all.score=all.score, DF_hat=DF_hat)
return(result)
}
#--------------------------------------------------/----------------------------------------
set.seed(30)
n=100
theta_ant = c(0,0,0)
y=matrix(,nrow=n,ncol=2)
for (i in seq(1,n)){
theta_i = theta_ant + rnorm(3,0,0.01)
y_1i =  theta_i[1] + rnorm(1,0,0.01)
y_2i = theta_i[2] + theta_i[3]*y_1i + rnorm(1,0,0.01)
y[i,] = c(y_1i,y_2i)
theta_ant = theta_i
}
y = data.frame(y)
colnames(y) = c('Y1','Y2')
cor(y)
Nn = ncol(y) # the number of nodes
Nm = 3 # the number of models
m_ad = array(0, dim=c(Nm,Nn,Nn))
# Model 1: node 1 -> node 2
m_ad[1,1,2]=1
# Model 2: node 1 <- node 2
m_ad[2,2,1]=1
# Choosing DF per node and per model
DF1 = CDELT(y,m_ad[1,,]) #Model 1: node 1 -> node 2
DF2 = CDELT(y,m_ad[2,,]) #Model 2: node 1 <- node 2
DF3 = CDELT(y,m_ad[3,,]) #Model 3: node 1    node 2
delta=seq(from=0.5, to=1.0, by=0.01)
plot(delta,rowSums(DF1$lpl), type="l", xlab="Discount Factor", ylab="LPL", main="Comparing 3 possible models for 2 nodes",xlim=c(0.5,1),ylim=c(-700,-500))
lines(delta,rowSums(DF2$lpl), lty = "dashed")
lines(delta,rowSums(DF3$lpl), lty = "dotted")
legend("bottomleft", legend = c("1->2","1<-2","1  2"), lty = c("solid", "dashed", "dotted"), bty = "n")
DF1$lpl
DF2$lpl
DF3$lpl
plot(delta,rowSums(DF1$lpl), type="l", xlab="Discount Factor", ylab="LPL", main="Comparing 3 possible models for 2 nodes",xlim=c(0.5,1),ylim=c(70,300))
lines(delta,rowSums(DF2$lpl), lty = "dashed")
lines(delta,rowSums(DF3$lpl), lty = "dotted")
legend("bottomleft", legend = c("1->2","1<-2","1  2"), lty = c("solid", "dashed", "dotted"), bty = "n")
plot(delta,rowSums(DF1$lpl), type="l", xlab="Discount Factor", ylab="LPL", main="Comparing 3 possible models for 2 nodes",xlim=c(0.5,1),ylim=c(70,500))
lines(delta,rowSums(DF2$lpl), lty = "dashed")
lines(delta,rowSums(DF3$lpl), lty = "dotted")
legend("bottomleft", legend = c("1->2","1<-2","1  2"), lty = c("solid", "dashed", "dotted"), bty = "n")
set.seed(30)
n=100
theta_ant = c(0,0,0)
y=matrix(,nrow=n,ncol=2)
for (i in seq(1,n)){
theta_i = theta_ant + rnorm(3,0,0.1)
y_1i =  theta_i[1] + rnorm(1,0,0.1)
y_2i = theta_i[2] + theta_i[3]*y_1i + rnorm(1,0,0.1)
y[i,] = c(y_1i,y_2i)
theta_ant = theta_i
}
y = data.frame(y)
colnames(y) = c('Y1','Y2')
cor(y)
Nn = ncol(y) # the number of nodes
Nm = 3 # the number of models
m_ad = array(0, dim=c(Nm,Nn,Nn))
# Model 1: node 1 -> node 2
m_ad[1,1,2]=1
# Model 2: node 1 <- node 2
m_ad[2,2,1]=1
# Choosing DF per node and per model
DF1 = CDELT(y,m_ad[1,,]) #Model 1: node 1 -> node 2
DF2 = CDELT(y,m_ad[2,,]) #Model 2: node 1 <- node 2
DF3 = CDELT(y,m_ad[3,,]) #Model 3: node 1    node 2
delta=seq(from=0.5, to=1.0, by=0.01)
plot(delta,rowSums(DF1$lpl), type="l", xlab="Discount Factor", ylab="LPL", main="Comparing 3 possible models for 2 nodes",xlim=c(0.5,1),ylim=c(70,500))
lines(delta,rowSums(DF2$lpl), lty = "dashed")
lines(delta,rowSums(DF3$lpl), lty = "dotted")
legend("bottomleft", legend = c("1->2","1<-2","1  2"), lty = c("solid", "dashed", "dotted"), bty = "n")
DF1$lpl
DF2$lpl
DF3$lpl
plot(delta,rowSums(DF1$lpl), type="l", xlab="Discount Factor", ylab="LPL", main="Comparing 3 possible models for 2 nodes",xlim=c(0.5,1),ylim=c(-200,30))
lines(delta,rowSums(DF2$lpl), lty = "dashed")
lines(delta,rowSums(DF3$lpl), lty = "dotted")
legend("bottomleft", legend = c("1->2","1<-2","1  2"), lty = c("solid", "dashed", "dotted"), bty = "n")
sim_score <- mdm_score(dts,GOLB_print = TRUE)
sim_score
set.seed(30)
n=100
theta_ant = c(0,0,0)
y=matrix(,nrow=n,ncol=2)
for (i in seq(1,n)){
theta_i = theta_ant + rnorm(3,0,0.1)
y_1i =  theta_i[1] + rnorm(1,0,0.1)
y_2i = theta_i[2] + theta_i[3]*y_1i + rnorm(1,0,0.1)
y[i,] = c(y_1i,y_2i)
theta_ant = theta_i
}
y = data.frame(y)
colnames(y) = c('Y1','Y2')
cor(y)
y
# Defining Adjacent Matrix
Nn = ncol(y) # the number of nodes
Nm = 3 # the number of models
m_ad = array(0, dim=c(Nm,Nn,Nn))
# Model 1: node 1 -> node 2
m_ad[1,1,2]=1
# Model 2: node 1 <- node 2
m_ad[2,2,1]=1
# Choosing DF per node and per model
DF1 = CDELT(y,m_ad[1,,]) #Model 1: node 1 -> node 2
DF2 = CDELT(y,m_ad[2,,]) #Model 2: node 1 <- node 2
DF3 = CDELT(y,m_ad[3,,]) #Model 3: node 1    node 2
delta=seq(from=0.5, to=1.0, by=0.01)
plot(delta,rowSums(DF1$lpl), type="l", xlab="Discount Factor", ylab="LPL", main="Comparing 3 possible models for 2 nodes",xlim=c(0.5,1),ylim=c(-200,30))
lines(delta,rowSums(DF2$lpl), lty = "dashed")
lines(delta,rowSums(DF3$lpl), lty = "dotted")
legend("bottomleft", legend = c("1->2","1<-2","1  2"), lty = c("solid", "dashed", "dotted"), bty = "n")
dts
y
sim_score <- mdm_score(dts,GOLB_print = TRUE)
sim_score
adj_matrix_sim <- mdmr::run_BN_pygobnilp('mdm_score_11_out_2023', palim_nodes  = 2)
dts = cbind(y1,y2)
# Defining Adjacent Matrix
Nn = ncol(dts) # the number of nodes
Nm = 3 # the number of models
m_ad = array(0, dim=c(Nm,Nn,Nn))
# Model 1: node 1 -> node 2
m_ad[1,1,2]=1
# Model 2: node 1 <- node 2
m_ad[2,2,1]=1
# Choosing DF per node and per model
DF1 = CDELT(dts,m_ad[1,,]) #Model 1: node 1 -> node 2
DF2 = CDELT(dts,m_ad[2,,]) #Model 2: node 1 <- node 2
DF3 = CDELT(dts,m_ad[3,,]) #Model 3: node 1    node 2
delta=seq(from=0.5, to=1.0, by=0.01)
plot(delta,rowSums(DF1$lpl), type="l", xlab="Discount Factor", ylab="LPL", main="Comparing 3 possible models for 2 nodes",xlim=c(0.5,1),ylim=c(-700,-500))
lines(delta,rowSums(DF2$lpl), lty = "dashed")
lines(delta,rowSums(DF3$lpl), lty = "dotted")
legend("bottomleft", legend = c("1->2","1<-2","1  2"), lty = c("solid", "dashed", "dotted"), bty = "n")
setwd("~/MLG_Idosos")
=======
digits = 4,
format.args=list(big.mark=".", decimal.mark=","),
align = "c",
row.names = T,
) %>%
kable_styling(
full_width = F, position = 'center',
latex_options = c("striped", "HOLD_position", "repeat_header")
)|>
column_spec(1, bold = T
)|>
kable_material()
#| echo: false
#| warning: false
Y <- dados$resp
X <- dados$score
n <- length(Y)
xx <- cbind(1,X)
phi <- n
# chute inicial mu = Y
mu <- Y
#### primeira iteração
eta <- Y
z <- eta
V <- diag((mu)*(1-mu))
W <- V
Kbb <- phi*t(xx)%*%W%*%xx
Kbb_inv <- solve(Kbb)
beta_it <- phi*Kbb_inv%*%t(xx)%*%W%*%z
phi
Kbb_inv
Kbb
xx
W
V
Y
(mu)*(1-mu)
# chute inicial mu = Y
mu <- as.numeric(Y)-1
mu
#### primeira iteração
eta <- mu
z <- eta
V <- diag((mu)*(1-mu))
W <- V
W
Kbb <- phi*t(xx)%*%W%*%xx
Kbb
Kbb <- phi*t(xx)%*%W%*%xx
Kbb_inv <- solve(Kbb)
#| echo: false
#| warning: false
Y <- dados$resp
X <- dados$score
n <- length(Y)
xx <- cbind(1,X)
phi <- n
#chute inicial
alpha <- 1
beta <- 0
# inicio das iterações
eta <- alpha + beta*X
mu <- exp(eta)/(1-exp(eta))
V <- diag((mu)*(1-mu))
W <- V
Kbb <- phi*t(xx)%*%W%*%xx
Kbb_inv <- solve(Kbb)
z <- eta + solve(V)%*%(Y-mu)
beta_it <- phi*Kbb_inv%*%t(xx)%*%W%*%z
beta_it
#| echo: false
#| warning: false
Y <- dados$resp
X <- dados$score
n <- length(Y)
xx <- cbind(1,X)
phi <- n
#chute inicial
alpha <- 1
beta <- 1
# inicio das iterações
eta <- alpha + beta*X
mu <- exp(eta)/(1-exp(eta))
V <- diag((mu)*(1-mu))
W <- V
Kbb <- phi*t(xx)%*%W%*%xx
Kbb_inv <- solve(Kbb)
z <- eta + solve(V)%*%(Y-mu)
beta_it <- phi*Kbb_inv%*%t(xx)%*%W%*%z
beta_it
#| echo: false
#| warning: false
Y <- dados$resp
X <- dados$score
n <- length(Y)
xx <- cbind(1,X)
phi <- n
#chute inicial
alpha <- 1
beta <- 0.1
# inicio das iterações
eta <- alpha + beta*X
mu <- exp(eta)/(1-exp(eta))
V <- diag((mu)*(1-mu))
W <- V
Kbb <- phi*t(xx)%*%W%*%xx
Kbb_inv <- solve(Kbb)
z <- eta + solve(V)%*%(Y-mu)
beta_it <- phi*Kbb_inv%*%t(xx)%*%W%*%z
beta_it
#chute inicial
alpha <- 1
beta <- 0.1
# inicio das iterações
eta <- alpha + beta*X
mu <- exp(eta)/(1-exp(eta))
V <- diag((mu)*(1-mu))
W <- V
Kbb <- phi*t(xx)%*%W%*%xx
Kbb_inv <- solve(Kbb)
z <- eta + solve(V)%*%(Y-mu)
Y <- as.numeric(dados$resp)-1
Y
n <- length(Y)
xx <- cbind(1,X)
phi <- n
#chute inicial
alpha <- 1
beta <- 0.1
# inicio das iterações
eta <- alpha + beta*X
mu <- exp(eta)/(1-exp(eta))
V <- diag((mu)*(1-mu))
W <- V
W
Kbb <- phi*t(xx)%*%W%*%xx
Kbb
Kbb_inv <- solve(Kbb)
Kbb_inv
z <- eta + solve(V)%*%(Y-mu)
z
beta_it <- phi*Kbb_inv%*%t(xx)%*%W%*%z
beta_it
#| echo: false
#| warning: false
Y <- as.numeric(dados$resp)-1
X <- dados$score
n <- length(Y)
xx <- cbind(1,X)
phi <- n
#chute inicial
alpha <- 1
beta <- 0.1
# inicio das iterações
while(TRUE){
eta <- alpha + beta*X
mu <- exp(eta)/(1-exp(eta))
V <- diag((mu)*(1-mu))
W <- V
Kbb <- phi*t(xx)%*%W%*%xx
Kbb_inv <- solve(Kbb)
z <- eta + solve(V)%*%(Y-mu)
beta_it <- phi*Kbb_inv%*%t(xx)%*%W%*%z
if(beta_it[1]-alfa<0.0001 & beta_it[2]-beta<0.0001) break
alpha <- beta_it[1]
beta <- beta_it[2]
}
#| echo: false
#| warning: false
Y <- as.numeric(dados$resp)-1
X <- dados$score
n <- length(Y)
xx <- cbind(1,X)
phi <- n
#chute inicial
alpha <- 1
beta <- 0.1
# inicio das iterações
while(TRUE){
eta <- alpha + beta*X
mu <- exp(eta)/(1-exp(eta))
V <- diag((mu)*(1-mu))
W <- V
Kbb <- phi*t(xx)%*%W%*%xx
Kbb_inv <- solve(Kbb)
z <- eta + solve(V)%*%(Y-mu)
beta_it <- phi*Kbb_inv%*%t(xx)%*%W%*%z
if(beta_it[1]-alpha<0.0001 & beta_it[2]-beta<0.0001) break
alpha <- beta_it[1]
beta <- beta_it[2]
}
#| echo: false
#| warning: false
Y <- as.numeric(dados$resp)-1
X <- dados$score
n <- length(Y)
xx <- cbind(1,X)
phi <- n
#chute inicial
alpha <- 1
beta <- 0.1
# inicio das iterações
while(TRUE){
eta <- alpha + beta*X
mu <- exp(eta)/(1-exp(eta))
V <- diag((mu)*(1-mu))
W <- V
Kbb <- phi*t(xx)%*%W%*%xx
Kbb_inv <- solve(Kbb)
z <- eta + solve(V)%*%(Y-mu)
beta_it <- phi*Kbb_inv%*%t(xx)%*%W%*%z
if(beta_it[1]-alpha<0.0001 & beta_it[2]-beta<0.0001) break
alpha <- beta_it[1]
beta <- beta_it[2]
}
beta_it
#| echo: false
#| warning: false
Y <- as.numeric(dados$resp)-1
X <- dados$score
n <- length(Y)
xx <- cbind(1,X)
phi <- n
#chute inicial
alpha <- 1
beta <- 0.1
# inicio das iterações
k <- 0
while(TRUE){
k <- k+1
eta <- alpha + beta*X
mu <- exp(eta)/(1-exp(eta))
V <- diag((mu)*(1-mu))
W <- V
Kbb <- phi*t(xx)%*%W%*%xx
Kbb_inv <- solve(Kbb)
z <- eta + solve(V)%*%(Y-mu)
beta_it <- phi*Kbb_inv%*%t(xx)%*%W%*%z
if(beta_it[1]-alpha<0.0001 & beta_it[2]-beta<0.0001) break
alpha <- beta_it[1]
beta <- beta_it[2]
}
beta_it
cat("Com",k,"iterações")
#| echo: false
#| warning: false
Y <- as.numeric(dados$resp)-1
X <- dados$score
n <- length(Y)
xx <- cbind(1,X)
phi <- n
#chute inicial
alpha <- 1
beta <- 0.1
# inicio das iterações
k <- 0
while(TRUE){
k <- k+1
eta <- alpha + beta*X
mu <- exp(eta)/(1-exp(eta))
V <- diag((mu)*(1-mu))
W <- V
Kbb <- phi*t(xx)%*%W%*%xx
Kbb_inv <- solve(Kbb)
z <- eta + solve(V)%*%(Y-mu)
beta_it <- phi*Kbb_inv%*%t(xx)%*%W%*%z
if(beta_it[1]-alpha<0.000001 & beta_it[2]-beta<0.000001) break
alpha <- beta_it[1]
beta <- beta_it[2]
}
beta_it
cat("Com",k,"iterações, atingiu a precisão de 10^-6")
D_est <- 2* sum(log(1/(n_i*mu_i)) + (n_i - 1)*log((1-(1/n_i))/(1-mu_i)))
#| echo: false
#| warning: false
library(pacman)
pacman::p_load(tidyverse,  readit, summarytools,
kableExtra,  ggpubr,  gridExtra,
glue, corrplot,  readxl, writexl, ggthemes,
patchwork,  plotly, gglm, ggplot2, tidymodels)
dados <- read_excel("idosos.xlsx")
dados$resp <- as.factor(dados$resp)
dados2 <- NULL
Com <- dados |> filter(resp==1)
Sem <- dados |> filter(resp==0)
#| echo: false
#| warning: false
#| mensage: false
tab_com <- Com|>
summarytools::descr(
stats = c("min", "q1", "med", "mean","q3", "max",  "sd", "cv", "Skewness", "Kurtosis"),
justify = "c",
style = "rmarkdown",
transpose = T)
tab_sem <- Sem|>
summarytools::descr(
stats = c("min", "q1", "med", "mean","q3", "max",  "sd", "cv", "Skewness", "Kurtosis"),
justify = "c",
style = "rmarkdown",
transpose = T)
tab <- rbind(tab_com,tab_sem)
rownames(tab) <- c("Com Caducância", "Sem Caducância")
tab|>
kbl(
caption = "Medidas resumo da variável score",
digits = 2,
format.args=list(big.mark=".", decimal.mark=","),
align = "c", row.names = T, booktabs = T
)|>
kable_styling(
full_width = F, position = 'center',
latex_options = c("striped", "HOLD_position", "scale_down", "repeat_header")
)|>
column_spec(1, bold = T
)|>
kable_material()
#| echo: false
#| warning: false
fit_l <- glm(resp~score, data=dados, family = binomial(link="logit"))
tab_l <- summary(fit_l)
tab_l <- tab_l$coefficients
rownames(tab_l) <- c("Intercepto", "Beta 1")
colnames(tab_l) <- c("Estimado", "SD", "z", "Pr(>|z|)")
parametros_l <- confint(fit_l)
#anova_l <- round(anova_l,3)
tab_l %>%
kbl(
caption = "Resultados da ANOVA, para o modelo com função de ligação logito.",
digits = 4,
format.args=list(big.mark=".", decimal.mark=","),
align = "c",
row.names = T,
) %>%
kable_styling(
full_width = F, position = 'center',
latex_options = c("striped", "HOLD_position", "repeat_header")
)|>
column_spec(1, bold = T
)|>
kable_material()
#| echo: false
#| warning: false
fit_p <- glm(resp~score, data=dados, family = binomial(link="probit"))
tab_p <- summary(fit_p)
tab_p <- tab_p$coefficients
rownames(tab_p) <- c("Intercepto", "Beta 1")
colnames(tab_p) <- c("Estimado", "SD", "z", "Pr(>|z|)")
parametros_p <- confint(fit_p)
tab_p %>%
kbl(
caption = "Resultados da ANOVA, para o modelo com função de ligação probito.",
digits = 4,
format.args=list(big.mark=".", decimal.mark=","),
align = "c",
row.names = T,
) %>%
kable_styling(
full_width = F, position = 'center',
latex_options = c("striped", "HOLD_position", "repeat_header")
)|>
column_spec(1, bold = T
)|>
kable_material()
#| echo: false
#| warning: false
fit_c <- glm(resp~score, data=dados, family = binomial(link="cauchit"))
tab_c <- summary(fit_c)
tab_c <- tab_c$coefficients
rownames(tab_c) <- c("Intercepto", "Beta 1")
colnames(tab_c) <- c("Estimado", "SD", "z", "Pr(>|z|)")
parametros_c <- confint(fit_c)
tab_c %>%
kbl(
caption = "Resultados da ANOVA, para o modelo com função de ligação cauchit.",
digits = 4,
format.args=list(big.mark=".", decimal.mark=","),
align = "c",
row.names = T,
) %>%
kable_styling(
full_width = F, position = 'center',
latex_options = c("striped", "HOLD_position", "repeat_header")
)|>
column_spec(1, bold = T
)|>
kable_material()
#| echo: false
#| warning: false
aic_l <- summary(fit_l)[5]
aic_p <- summary(fit_p)[5]
aic_c <- summary(fit_c)[5]
aic_l <- as.numeric(aic_l)
aic_p <- as.numeric(aic_p)
aic_c <- as.numeric(aic_c)
aic_l <- round(aic_l,4)
aic_p <- round(aic_p,4)
aic_c <- round(aic_c,4)
tab <- cbind(c("Logito", "Probito", "Cauchit"), c(aic_l,aic_p,aic_c))
colnames(tab) <- c("Função", "AIC")
tab %>%
kbl(
caption = "Valores de AIC dos modelos da ANOVA, para cada função de ligação.",
digits = 4,
format.args=list(big.mark=".", decimal.mark=","),
align = "c",
row.names = F,
) %>%
kable_styling(
full_width = F, position = 'center',
latex_options = c("striped", "HOLD_position", "repeat_header")
)|>
column_spec(1, bold = T
)|>
kable_material()
#| echo: false
#| warning: false
Y <- as.numeric(dados$resp)-1
X <- dados$score
n <- length(Y)
xx <- cbind(1,X)
phi <- n
#chute inicial
alpha <- 1
beta <- 0.1
# inicio das iterações
k <- 0
while(TRUE){
k <- k+1
eta <- alpha + beta*X
mu <- exp(eta)/(1-exp(eta))
V <- diag((mu)*(1-mu))
W <- V
Kbb <- phi*t(xx)%*%W%*%xx
Kbb_inv <- solve(Kbb)
z <- eta + solve(V)%*%(Y-mu)
beta_it <- phi*Kbb_inv%*%t(xx)%*%W%*%z
if(beta_it[1]-alpha<0.000001 & beta_it[2]-beta<0.000001) break
alpha <- beta_it[1]
beta <- beta_it[2]
}
# cálculo da função desvio
# Yi = 0 (sem) ou 1(com)
n_i <- length(Com$score)
mu_i <- mean(Com$score)
D_est <- 2* sum(log(1/(n_i*mu_i)) + (n_i - 1)*log((1-(1/n_i))/(1-mu_i)))
D_est
log(1/(n_i*mu_i)
log(1/(n_i*mu_i))
log(1/(n_i*mu_i))
(n_i - 1)
((1-(1/n_i))/(1-mu_i))
log((1-(1/n_i))/(1-mu_i))
1/n_i
mu_i
dados
#| echo: false
#| warning: false
library(pacman)
pacman::p_load(tidyverse,  readit, summarytools,
kableExtra,  ggpubr,  gridExtra,
glue, corrplot,  readxl, writexl, ggthemes,
patchwork,  plotly, gglm, ggplot2, tidymodels)
dados <- read_excel("idosos.xlsx")
dados$resp <- as.factor(dados$resp)
dados2 <- NULL
Com <- dados |> filter(resp==1)
Sem <- dados |> filter(resp==0)
#| echo: false
#| warning: false
#| mensage: false
tab_com <- Com|>
summarytools::descr(
stats = c("min", "q1", "med", "mean","q3", "max",  "sd", "cv", "Skewness", "Kurtosis"),
justify = "c",
style = "rmarkdown",
transpose = T)
tab_sem <- Sem|>
summarytools::descr(
stats = c("min", "q1", "med", "mean","q3", "max",  "sd", "cv", "Skewness", "Kurtosis"),
justify = "c",
style = "rmarkdown",
transpose = T)
tab <- rbind(tab_com,tab_sem)
rownames(tab) <- c("Com Caducância", "Sem Caducância")
tab|>
kbl(
caption = "Medidas resumo da variável score",
digits = 2,
format.args=list(big.mark=".", decimal.mark=","),
align = "c", row.names = T, booktabs = T
)|>
kable_styling(
full_width = F, position = 'center',
latex_options = c("striped", "HOLD_position", "scale_down", "repeat_header")
)|>
column_spec(1, bold = T
)|>
kable_material()
#| echo: false
#| warning: false
fit_l <- glm(resp~score, data=dados, family = poisson(link = "log"))
#| echo: false
#| warning: false
dados$resp <- dados$resp |> filter(resp==0) |> mutate(resp=0.00000001)
#| echo: false
#| warning: false
dados$resp <- dados$resp |> filter(resp==0) |> mutate(resp=0.00000001)
#| echo: false
#| warning: false
dados$resp <- dados$resp |> filter(dados$resp==0) |> mutate(resp=0.00000001)
fit_l <- glm(as.factor(resp)~score, data=dados, family = poisson(link = "log"))
fit_l <- glm((resp+1)~score, data=dados, family = poisson(link = "log"))
>>>>>>> 3c623c3f0135258b5af06809c2c58f9319db4c23
